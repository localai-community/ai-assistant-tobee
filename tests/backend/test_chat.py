#!/usr/bin/env python3
"""
LocalAI Community - Chat Service Test
Tests the chat service and Ollama integration.
"""

import asyncio
import sys
import os

# Path is set by the test runner

from app.services.chat import ChatService, ChatRequest
import httpx

async def test_ollama_connection():
    """Test connection to Ollama."""
    print("ğŸ”— Testing Ollama Connection...")
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
            if response.status_code == 200:
                data = response.json()
                models = [model["name"] for model in data.get("models", [])]
                print(f"  âœ… Ollama is running")
                print(f"  ğŸ“‹ Available models: {models}")
                return True, models
            else:
                print(f"  âŒ Ollama responded with status: {response.status_code}")
                return False, []
    except Exception as e:
        print(f"  âŒ Failed to connect to Ollama: {e}")
        return False, []

async def test_chat_service():
    """Test the chat service."""
    print("\nğŸ¤– Testing Chat Service...")
    
    try:
        async with ChatService() as chat_service:
            # Test health check
            print("  ğŸ” Testing health check...")
            health = await chat_service.check_ollama_health()
            if health:
                print("    âœ… Health check passed")
            else:
                print("    âŒ Health check failed")
                return False
            
            # Test model listing
            print("  ğŸ“‹ Testing model listing...")
            models = await chat_service.get_available_models()
            if models:
                print(f"    âœ… Found {len(models)} models: {models}")
            else:
                print("    âš ï¸  No models found")
            
            # Test basic chat (if models available)
            if models:
                print("  ğŸ’¬ Testing basic chat...")
                try:
                    response = await chat_service.generate_response(
                        message="Hello! Please respond with 'Hello from Ollama!'",
                        model=models[0],
                        temperature=0.1
                    )
                    print(f"    âœ… Chat response: {response.response[:100]}...")
                    print(f"    ğŸ“Š Conversation ID: {response.conversation_id}")
                    return True
                except Exception as e:
                    print(f"    âŒ Chat test failed: {e}")
                    return False
            else:
                print("    âš ï¸  Skipping chat test (no models available)")
                return True
                
    except Exception as e:
        print(f"  âŒ Chat service test failed: {e}")
        return False

async def test_chat_api():
    """Test the chat API endpoints."""
    print("\nğŸŒ Testing Chat API...")
    
    try:
        async with httpx.AsyncClient() as client:
            base_url = "http://localhost:8000"
            
            # Test health endpoint
            print("  ğŸ” Testing API health...")
            response = await client.get(f"{base_url}/api/v1/chat/health")
            if response.status_code == 200:
                data = response.json()
                print(f"    âœ… API health: {data}")
            else:
                print(f"    âŒ API health failed: {response.status_code}")
                return False
            
            # Test models endpoint
            print("  ğŸ“‹ Testing models endpoint...")
            response = await client.get(f"{base_url}/api/v1/chat/models")
            if response.status_code == 200:
                models = response.json()
                print(f"    âœ… Available models: {models}")
            else:
                print(f"    âŒ Models endpoint failed: {response.status_code}")
                return False
            
            # Test chat endpoint (if models available)
            if models:
                print("  ğŸ’¬ Testing chat endpoint...")
                chat_request = {
                    "message": "Hello! Please respond with 'Hello from API!'",
                    "model": models[0],
                    "temperature": 0.1
                }
                response = await client.post(f"{base_url}/api/v1/chat/", json=chat_request)
                if response.status_code == 200:
                    data = response.json()
                    print(f"    âœ… Chat response: {data['response'][:100]}...")
                    print(f"    ğŸ“Š Conversation ID: {data['conversation_id']}")
                    return True
                else:
                    print(f"    âŒ Chat endpoint failed: {response.status_code}")
                    print(f"    ğŸ“„ Response: {response.text}")
                    return False
            else:
                print("    âš ï¸  Skipping chat endpoint test (no models available)")
                return True
                
    except Exception as e:
        print(f"  âŒ Chat API test failed: {e}")
        return False

async def main():
    """Main test function."""
    print("ğŸ§ª LocalAI Community - Chat Service Test")
    print("=" * 50)
    
    # Test Ollama connection
    ollama_ok, models = await test_ollama_connection()
    
    if not ollama_ok:
        print("\nâŒ Ollama is not running or not accessible.")
        print("   Please start Ollama first:")
        print("   ollama serve")
        print("   ollama pull llama3.2")
        return False
    
    # Test chat service
    service_ok = await test_chat_service()
    
    # Test chat API (if backend is running)
    api_ok = await test_chat_api()
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 50)
    print(f"ğŸ”— Ollama Connection: {'âœ…' if ollama_ok else 'âŒ'}")
    print(f"ğŸ¤– Chat Service: {'âœ…' if service_ok else 'âŒ'}")
    print(f"ğŸŒ Chat API: {'âœ…' if api_ok else 'âŒ'}")
    
    if ollama_ok and service_ok:
        print("\nğŸ‰ Chat functionality is working!")
        return True
    else:
        print("\nâŒ Some tests failed. Please check the errors above.")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1) 